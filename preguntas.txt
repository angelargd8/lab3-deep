--- Parte 1 ---

* ¿Qué interpretación le da a la separación de las graficas de training y validation?
Lo que se logra observar en el gráfico es que en un principio, las dos curvas bajan rápidamente, que es dónde se está apredniendo patrones de las secuencias. 
Ya luego, cuando llega a las 200 épocas se ve que el training baja más rápido que el validation, entonces el modelo parece que aún está generalizando, 
pero el aprendizaje es mayor en el de training. Al final, lo que se logra notar, por la época 1600, es que la curva de training sigue bajando, 
pero la de validation parece estar un poco estancada y tener algunas oscilaciones. 

* ¿Cree que es un buen modelo basado solamente en el loss?
Si es un buen modelo, ya que la validación y el entrenamiento van disminuyendo juntos en la parte del entrenamiento. 
Por otro lado, la validación no se vuelve ascendente o tiene una divergencia fuerte.

* ¿Cómo deberían de verse esas gráficas en un modelo ideal?
Un modelo ideal, se vería con curvas paralelas, teniendo una pequeña separación que es constante. 
Además, estás deberían estabilizarse juntas. Y por último, no tendrían oscilaciones fuertes en la validación. 

--- Parte 2 ---

* ¿Qué modelo funcionó mejor? ¿RNN tradicional o el basado en LSTM? ¿Por qué?
Se puede observar como el modelo LSTM es mejor en la perdida de entrenamiento, tiene valores mas bajos y se ve mas estable, lo cual indica una mejor capacidad de generalización.
Por el otro lado, la curva de validacion de LSTM es variable, pueden estar afectando la sensibilidad del modelo a los datos. 
Puede que el LSTM este diseñado para capturar dependencias a largo plazo, pero su entrenamiento puede ser más delicado y propenso a oscilaciones

* Observen la gráfica obtenida arriba, ¿en qué es diferente a la obtenida a RNN? ¿Es esto mejor o peor? ¿Por qué?
La RNN tiene una perdida de entrenamineto y de validacion que disminuyen con mas consistencia pero esta pareciera que tiende a aumentar,
teniendo resultados mas altos que los de la grafica de LSTM.
En cambio en la LSTM se puede observar como al incio es muy variable pero luego tiende a mejorar sus valores siendo mas bajos que los de RNN. Ademas,
se puede observar como la curva de entrenamiento es mejor.

* ¿Por qué LSTM puede funcionar mejor con secuencias largas?
LSTM esta diseñada para manerjar secuencias a largo plazo por la forma en la que esta hecha donde cada uno de sus inputs lo ayudan a evitar
que el gradiente desvanecido. Que suele pasar mucho en RNN, lo que limita la capacidad de aprender.

--- Parte 3 ---

* Compare las graficas obtenidas en el LSTM "a mano" y el LSTM "usando PyTorch, ¿cuál cree que es mejor? ¿Por qué?
El gráfico obtenido con PyTorch es el mejor, ya que este converge de una manera más rápida, tiene una mejor generalización, lo que significa menos overfitting, tiene una mejor estabilidad de curvas a comparación de la hecha con numpy y también al final tiene una menor pérdida. 

* Compare la secuencia target y la predicha de esta parte, ¿en qué parte falló el modelo?
En la posicion número 9 se esperaba una a, pero es una b. Luego predijo una b estra porque empezó antes. 


* ¿Qué sucede en el código donde se señala "NOTA 1" y "NOTA 2"? ¿Para qué son necesarias estas líneas?
La nota 1 dice eval, que es la nota para el modo de evaluación, mientras que la nota 2 es para el modo de entrenamiento, lo que hacen esas notas es controlar el comportamiento interno del modelo. Son necesarias, porque puede que se usen en el orden incorrecto y el entrenamiento será inconsistente y el modelo no aprenderá bien. 